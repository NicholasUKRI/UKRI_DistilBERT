{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Import"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from typing import Optional\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    Trainer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    )\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "from typing import List\n",
    "import re\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data and remove grants not appropriate for text analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "49602"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "with open(\"data\\\\projects.json\", \"r\") as f:\n",
    "  dataPrep = json.load(f)\n",
    "\n",
    "# remove unwanted grants\n",
    "def remove_from_dict(\n",
    "    key_name: str,\n",
    "    values: List[str],\n",
    "    dictionary: dict,\n",
    "    use_regex: Optional[bool] = False,\n",
    "):\n",
    "    values = [value.casefold() for value in values]\n",
    "\n",
    "    regular_comparator = lambda string: string.casefold() not in values\n",
    "    regex_comparator = lambda string: not any(\n",
    "        re.match(value, string.casefold()) for value in values\n",
    "    )\n",
    "\n",
    "    comparator = regex_comparator if use_regex else regular_comparator\n",
    "\n",
    "    filtered_dict = {\n",
    "        key: inner_dict\n",
    "        for key, inner_dict in dictionary.items()\n",
    "        if comparator(inner_dict[key_name])\n",
    "    }\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "def remove_from_dict_int(key_name: str, values: int, dictionary: dict):\n",
    "\n",
    "    filtered_dict = {\n",
    "        key: inner_dict\n",
    "        for key, inner_dict in dictionary.items()\n",
    "        if inner_dict[key_name] not in values\n",
    "    }\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "def remove_from_dict_small(key_name: str, value: int, dictionary: dict):\n",
    "\n",
    "    filtered_dict = {\n",
    "        key: inner_dict\n",
    "        for key, inner_dict in dictionary.items()\n",
    "        if len(inner_dict[key_name]) > value\n",
    "    }\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "dataPrep = remove_from_dict(\"title\", [\"dtp\"], dataPrep, use_regex=True)\n",
    "dataPrep = remove_from_dict(\"abstract\", [\"Summary: Equipment only\", \"Abstracts are not currently available in GtR\", \"As per advice from EPSRC, please see the attachments\",\n",
    "                        \"Refer to ATLAS-UK\", \"Abstract from the telescope proposal\", \"Doctoral Training Partnerships: a range of postgraduate training\", \"See Je-S application\", \"As agreed with AHRC please see\", \"Awaiting Public Project Summary\", \"no public description\", \"The public description for this project has been requested but has not yet been received\", \"No abstract available\", \"Equipment only, agreed in relation to the previously issued GridPP4 grant\", \"Equipment only, agreed in relation to the previously issued GridPP5 grant\"], dataPrep, use_regex=True)\n",
    "dataPrep = remove_from_dict(\"abstract\", [\"none\"], dataPrep, use_regex=False)\n",
    "\n",
    "dataPrep = remove_from_dict_small(\"abstract\", 75, dataPrep)\n",
    "\n",
    "# remove anything with 0 funding value (mostly studentships)\n",
    "dataPrep = remove_from_dict_int(\"funding\", [0], dataPrep)\n",
    "\n",
    "# remove duplicates in abstract/title. An argument could be made for keeping them in, but I think it would skew the results due to transferred grants etc\n",
    "seen = set()\n",
    "data = {}\n",
    "for k, v in dataPrep.items():\n",
    "    if (v['title'], v['abstract']) not in seen:\n",
    "        data[k] = v\n",
    "        seen.add((v['title'], v['abstract']))\n",
    "\n",
    "df = pd.DataFrame(data).transpose()\n",
    "\n",
    "# final bit of tidying. data also goes back to 1991 which will skew language (maybe), and too much data.\n",
    "df = df.dropna()\n",
    "df['start'] = pd.to_datetime(df['start'])\n",
    "df['end'] = pd.to_datetime(df['end'])\n",
    "df['funding'] = pd.to_numeric(df['funding'])\n",
    "\n",
    "df = df[(df[\"start\"] >\"2013-01-01\")]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# I want to group up the unique project titles/abstracts\n",
    "uniques = df.groupby(['title', 'abstract'], as_index=False).agg({'ref': 'sum',\n",
    "                                                                         'funding': 'sum',\n",
    "                                                                         'start': lambda x: x.iloc[0],\n",
    "                                                                         'end': lambda x: x.iloc[0],\n",
    "                                                                         'funder': lambda x: x.iloc[0],\n",
    "                                                                         'category': lambda x: x.iloc[0]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean text + merge title/abstract"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "#Clean abstracts\n",
    "lang_map = {\n",
    "    \"\\n\": \"|\",\n",
    "    \"&quot;\": '\"',\n",
    "    \"&amp;\": \"&\",\n",
    "    \"|\": \"\",\n",
    "}\n",
    "\n",
    "def clean_up(text, lang_map=lang_map):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    for key in lang_map:\n",
    "        text = text.replace(key, lang_map[key])\n",
    "    return text.strip()\n",
    "\n",
    "uniques['abstract'] = uniques['abstract'].apply(lambda x: clean_up(x))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "#Embedding Functions\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"\n",
    "    Effectively averages the embeddings of tokens across the vocabulary dimension\n",
    "    to calculate the vocab-weighted latent representations (embeddings).\n",
    "\n",
    "    :param token_embeddings: torch.float tensor of size (n_examples, n_vocab, n_latent)\n",
    "    :param attention_mask: torch.byte tensor of size (n_examples, n_vocab)\n",
    "    :return: torch.float tensor of size (n_examples, n_latent)\n",
    "    \"\"\"\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    out = sum_embeddings / sum_mask\n",
    "    del input_mask_expanded\n",
    "    del sum_embeddings\n",
    "    del sum_mask\n",
    "    return out.squeeze()\n",
    "\n",
    "def embed(text, model, tokenizer):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(torch.device(\"cuda\"))\n",
    "    embeddings = model(**tokens, output_hidden_states=True).hidden_states[-1]\n",
    "    pooled = mean_pooling(embeddings, tokens['attention_mask']).detach().cpu()\n",
    "    dim_size = len(text) if isinstance(text, list) else 1\n",
    "    del tokens\n",
    "    del embeddings\n",
    "    return pooled.reshape(dim_size, -1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "uniques.to_json(path_or_buf=\"data\\\\metadata.json\", orient='index')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c45134bc7162ebad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:\\Users\\n3hoo\\.cache\\huggingface\\datasets\\json\\default-c45134bc7162ebad\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "034369c06ef34ad38ec8edfd1ffd15ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d98caf4df4641c68d0b89044da0058e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0 tables [00:00, ? tables/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b7748f27a714cdf9c23adbd8995f923"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:\\Users\\n3hoo\\.cache\\huggingface\\datasets\\json\\default-c45134bc7162ebad\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b71de12e4e41438d9c596ae616e4f460"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"data\\\\metadata.json\", \"r\") as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "for entry in data:\n",
    "  data[entry]['id'] = entry\n",
    "\n",
    "# join abstract + title together\n",
    "fields = [\"abstract\", \"title\"]\n",
    "target = \"abstract_title_merge\"\n",
    "for k, v in data.items():\n",
    "    v[target] = \" \".join(v[n] for n in fields)\n",
    "\n",
    "dictlist = {\"data\": [{'abstract_title_merge': data[id]['abstract_title_merge']} for id in data.keys()]}\n",
    "\n",
    "with open(\"data\\\\datalist.json\", \"w\") as f:\n",
    "  json.dump(dictlist, f)\n",
    "\n",
    "data = load_dataset('json', data_files=\"data\\\\datalist.json\", field=\"data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import distilbert models + create training dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/38 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "704bdeb7062c438cad78f6c9b810ad77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/13 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a95da30329344ddbf8d048a6720c79e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name], return_special_tokens_mask=True, truncation=True)  # careful of NaNs in your metadata (Nick H), it will throw errors\n",
    "\n",
    "data = data['train'].train_test_split(test_size=0.25)\n",
    "\n",
    "text_column_name = \"abstract_title_merge\"\n",
    "NUM_WORKERS = None\n",
    "overwrite_cache = False\n",
    "tokenized_datasets = data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_WORKERS,\n",
    "    remove_columns=[text_column_name],\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    ")\n",
    "\n",
    "# data collation\n",
    "pad_to_multiple_of_8 = True\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    "    pad_to_multiple_of=8 if pad_to_multiple_of_8 else None,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# set up trainer\n",
    "\n",
    "training_args = TrainingArguments(do_train=True,\n",
    "                                  do_eval=True,\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  output_dir=\"tmp_trainer\",\n",
    "                                  num_train_epochs=10,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  logging_steps=5000,\n",
    "                                  eval_steps=5000,\n",
    "                                  evaluation_strategy=\"steps\")\n",
    "\n",
    "train_dataset = tokenized_datasets['train']\n",
    "eval_dataset = tokenized_datasets['test']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "!WARNING!... the next part takes a long time. Also, the temp folders for fine-tuning needs AT least 185Gb of space when pulling all GTR data as of September 2022."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "C:\\Users\\n3hoo\\PycharmProjects\\gtr_extract\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 37201\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 46510\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='46510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/46510 : < :, Epoch 0.00/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to tmp_trainer\\checkpoint-500\n",
      "Configuration saved in tmp_trainer\\checkpoint-500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-1000\n",
      "Configuration saved in tmp_trainer\\checkpoint-1000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-1500\n",
      "Configuration saved in tmp_trainer\\checkpoint-1500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-2000\n",
      "Configuration saved in tmp_trainer\\checkpoint-2000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-2000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-2500\n",
      "Configuration saved in tmp_trainer\\checkpoint-2500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-2500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-3000\n",
      "Configuration saved in tmp_trainer\\checkpoint-3000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-3500\n",
      "Configuration saved in tmp_trainer\\checkpoint-3500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-3500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-4000\n",
      "Configuration saved in tmp_trainer\\checkpoint-4000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-4000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-4500\n",
      "Configuration saved in tmp_trainer\\checkpoint-4500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-4500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-5000\n",
      "Configuration saved in tmp_trainer\\checkpoint-5000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-5500\n",
      "Configuration saved in tmp_trainer\\checkpoint-5500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-5500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-6000\n",
      "Configuration saved in tmp_trainer\\checkpoint-6000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-6000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-6500\n",
      "Configuration saved in tmp_trainer\\checkpoint-6500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-6500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-7000\n",
      "Configuration saved in tmp_trainer\\checkpoint-7000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-7000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-7500\n",
      "Configuration saved in tmp_trainer\\checkpoint-7500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-7500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-8000\n",
      "Configuration saved in tmp_trainer\\checkpoint-8000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-8000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-8500\n",
      "Configuration saved in tmp_trainer\\checkpoint-8500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-8500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-9000\n",
      "Configuration saved in tmp_trainer\\checkpoint-9000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-9000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-9500\n",
      "Configuration saved in tmp_trainer\\checkpoint-9500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-9500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-10000\n",
      "Configuration saved in tmp_trainer\\checkpoint-10000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-10500\n",
      "Configuration saved in tmp_trainer\\checkpoint-10500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-10500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-11000\n",
      "Configuration saved in tmp_trainer\\checkpoint-11000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-11000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-11500\n",
      "Configuration saved in tmp_trainer\\checkpoint-11500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-11500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-12000\n",
      "Configuration saved in tmp_trainer\\checkpoint-12000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-12000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-12500\n",
      "Configuration saved in tmp_trainer\\checkpoint-12500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-12500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-12500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-12500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-13000\n",
      "Configuration saved in tmp_trainer\\checkpoint-13000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-13000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-13500\n",
      "Configuration saved in tmp_trainer\\checkpoint-13500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-13500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-13500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-13500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-14000\n",
      "Configuration saved in tmp_trainer\\checkpoint-14000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-14000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-14500\n",
      "Configuration saved in tmp_trainer\\checkpoint-14500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-14500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-14500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-14500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-15000\n",
      "Configuration saved in tmp_trainer\\checkpoint-15000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-15500\n",
      "Configuration saved in tmp_trainer\\checkpoint-15500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-15500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-15500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-15500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-16000\n",
      "Configuration saved in tmp_trainer\\checkpoint-16000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-16000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-16500\n",
      "Configuration saved in tmp_trainer\\checkpoint-16500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-16500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-16500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-16500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-17000\n",
      "Configuration saved in tmp_trainer\\checkpoint-17000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-17000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-17500\n",
      "Configuration saved in tmp_trainer\\checkpoint-17500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-17500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-17500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-17500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-18000\n",
      "Configuration saved in tmp_trainer\\checkpoint-18000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-18000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-18500\n",
      "Configuration saved in tmp_trainer\\checkpoint-18500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-18500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-18500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-18500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-19000\n",
      "Configuration saved in tmp_trainer\\checkpoint-19000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-19000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-19000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-19000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-19500\n",
      "Configuration saved in tmp_trainer\\checkpoint-19500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-19500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-19500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-19500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-20000\n",
      "Configuration saved in tmp_trainer\\checkpoint-20000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-20000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-20500\n",
      "Configuration saved in tmp_trainer\\checkpoint-20500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-20500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-20500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-20500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-21000\n",
      "Configuration saved in tmp_trainer\\checkpoint-21000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-21000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-21000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-21000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-21500\n",
      "Configuration saved in tmp_trainer\\checkpoint-21500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-21500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-21500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-21500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-22000\n",
      "Configuration saved in tmp_trainer\\checkpoint-22000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-22000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-22000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-22000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-22500\n",
      "Configuration saved in tmp_trainer\\checkpoint-22500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-22500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-22500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-22500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-23000\n",
      "Configuration saved in tmp_trainer\\checkpoint-23000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-23000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-23000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-23000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-23500\n",
      "Configuration saved in tmp_trainer\\checkpoint-23500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-23500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-23500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-23500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-24000\n",
      "Configuration saved in tmp_trainer\\checkpoint-24000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-24000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-24500\n",
      "Configuration saved in tmp_trainer\\checkpoint-24500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-24500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-24500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-24500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-25000\n",
      "Configuration saved in tmp_trainer\\checkpoint-25000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-25000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-25000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-25000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-25500\n",
      "Configuration saved in tmp_trainer\\checkpoint-25500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-25500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-25500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-25500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-26000\n",
      "Configuration saved in tmp_trainer\\checkpoint-26000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-26000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-26000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-26000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-26500\n",
      "Configuration saved in tmp_trainer\\checkpoint-26500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-26500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-26500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-26500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-27000\n",
      "Configuration saved in tmp_trainer\\checkpoint-27000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-27000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-27000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-27000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-27500\n",
      "Configuration saved in tmp_trainer\\checkpoint-27500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-27500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-27500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-27500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-28000\n",
      "Configuration saved in tmp_trainer\\checkpoint-28000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-28000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-28000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-28000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-28500\n",
      "Configuration saved in tmp_trainer\\checkpoint-28500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-28500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-28500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-28500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-29000\n",
      "Configuration saved in tmp_trainer\\checkpoint-29000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-29000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-29000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-29000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-29500\n",
      "Configuration saved in tmp_trainer\\checkpoint-29500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-29500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-29500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-29500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-30000\n",
      "Configuration saved in tmp_trainer\\checkpoint-30000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-30000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-30500\n",
      "Configuration saved in tmp_trainer\\checkpoint-30500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-30500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-30500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-30500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-31000\n",
      "Configuration saved in tmp_trainer\\checkpoint-31000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-31000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-31000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-31000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-31500\n",
      "Configuration saved in tmp_trainer\\checkpoint-31500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-31500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-31500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-31500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-32000\n",
      "Configuration saved in tmp_trainer\\checkpoint-32000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-32000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-32000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-32000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-32500\n",
      "Configuration saved in tmp_trainer\\checkpoint-32500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-32500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-32500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-32500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-33000\n",
      "Configuration saved in tmp_trainer\\checkpoint-33000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-33000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-33000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-33000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-33500\n",
      "Configuration saved in tmp_trainer\\checkpoint-33500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-33500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-33500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-33500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-34000\n",
      "Configuration saved in tmp_trainer\\checkpoint-34000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-34000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-34000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-34000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-34500\n",
      "Configuration saved in tmp_trainer\\checkpoint-34500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-34500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-34500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-34500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-35000\n",
      "Configuration saved in tmp_trainer\\checkpoint-35000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-35000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-35000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-35000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-35500\n",
      "Configuration saved in tmp_trainer\\checkpoint-35500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-35500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-35500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-35500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-36000\n",
      "Configuration saved in tmp_trainer\\checkpoint-36000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-36000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-36000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-36000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-36500\n",
      "Configuration saved in tmp_trainer\\checkpoint-36500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-36500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-36500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-36500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-37000\n",
      "Configuration saved in tmp_trainer\\checkpoint-37000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-37000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-37000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-37000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-37500\n",
      "Configuration saved in tmp_trainer\\checkpoint-37500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-37500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-37500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-37500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-38000\n",
      "Configuration saved in tmp_trainer\\checkpoint-38000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-38000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-38000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-38000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-38500\n",
      "Configuration saved in tmp_trainer\\checkpoint-38500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-38500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-38500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-38500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-39000\n",
      "Configuration saved in tmp_trainer\\checkpoint-39000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-39000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-39000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-39000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-39500\n",
      "Configuration saved in tmp_trainer\\checkpoint-39500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-39500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-39500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-39500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-40000\n",
      "Configuration saved in tmp_trainer\\checkpoint-40000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-40000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-40500\n",
      "Configuration saved in tmp_trainer\\checkpoint-40500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-40500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-40500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-40500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-41000\n",
      "Configuration saved in tmp_trainer\\checkpoint-41000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-41000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-41000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-41000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-41500\n",
      "Configuration saved in tmp_trainer\\checkpoint-41500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-41500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-41500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-41500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-42000\n",
      "Configuration saved in tmp_trainer\\checkpoint-42000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-42000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-42000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-42000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-42500\n",
      "Configuration saved in tmp_trainer\\checkpoint-42500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-42500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-42500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-42500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-43000\n",
      "Configuration saved in tmp_trainer\\checkpoint-43000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-43000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-43000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-43000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-43500\n",
      "Configuration saved in tmp_trainer\\checkpoint-43500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-43500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-43500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-43500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-44000\n",
      "Configuration saved in tmp_trainer\\checkpoint-44000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-44000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-44000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-44000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-44500\n",
      "Configuration saved in tmp_trainer\\checkpoint-44500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-44500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-44500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-44500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12401\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-45000\n",
      "Configuration saved in tmp_trainer\\checkpoint-45000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-45000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-45000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-45000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-45500\n",
      "Configuration saved in tmp_trainer\\checkpoint-45500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-45500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-45500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-45500\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-46000\n",
      "Configuration saved in tmp_trainer\\checkpoint-46000\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-46000\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-46000\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-46000\\special_tokens_map.json\n",
      "Saving model checkpoint to tmp_trainer\\checkpoint-46500\n",
      "Configuration saved in tmp_trainer\\checkpoint-46500\\config.json\n",
      "Model weights saved in tmp_trainer\\checkpoint-46500\\pytorch_model.bin\n",
      "tokenizer config file saved in tmp_trainer\\checkpoint-46500\\tokenizer_config.json\n",
      "Special tokens file saved in tmp_trainer\\checkpoint-46500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to model\\distilbert_ukri\n",
      "Configuration saved in model\\distilbert_ukri\\config.json\n",
      "Model weights saved in model\\distilbert_ukri\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\distilbert_ukri\\tokenizer_config.json\n",
      "Special tokens file saved in model\\distilbert_ukri\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# create data folder for data\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"model\\\\distilbert_ukri\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import fine-tunel model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file model\\distilbert_ukri/added_tokens.json. We won't load it.\n",
      "loading file model\\distilbert_ukri/vocab.txt\n",
      "loading file model\\distilbert_ukri/tokenizer.json\n",
      "loading file None\n",
      "loading file model\\distilbert_ukri/special_tokens_map.json\n",
      "loading file model\\distilbert_ukri/tokenizer_config.json\n",
      "loading configuration file model\\distilbert_ukri/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"model\\\\distilbert_ukri/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model\\distilbert_ukri/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at model\\distilbert_ukri/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"model\\\\distilbert_ukri/\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"model\\\\distilbert_ukri/\")\n",
    "model = model.to(torch.device('cuda'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49602/49602 [18:14<00:00, 45.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 24min 57s\n",
      "Wall time: 18min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "M = torch.zeros([1, 768])\n",
    "\n",
    "for _batch in tqdm.tqdm(range(0, uniques.shape[0]), position=0, leave=True):  # force restart\n",
    "    text = uniques.abstract.iloc[_batch]\n",
    "    embeddings = embed(text, model, tokenizer)\n",
    "    M = torch.cat((M, embeddings.detach().cpu()), dim=0)\n",
    "    # torch.save(embeddings.half(), DIR + f\"/batch_{last_max}.pt\")\n",
    "    del embeddings\n",
    "    del text\n",
    "\n",
    "torch.save(M[1:], \"data\\\\distilbert_ukri_tensor.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}